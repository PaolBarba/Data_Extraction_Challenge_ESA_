# Data Discovery Challenge - ESA

A modular pipeline for dataset discovery and submission, built for the European Space Agency's data challenge. This project integrates scraping, prompt engineering, model interaction, and a submission framework.

---

## ðŸ“ Project Structure

```text

Data_Discovery_Challenge_ESA_/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Data_Discovery/
â”‚   â”‚   â”œâ”€â”€ cleaning/                   # Cleaning files
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â”œâ”€â”€ config/                     # Configuration files
â”‚   â”‚   â”‚   â”œâ”€â”€ data_preparation_config/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ model_config/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .env
â”‚   â”‚   â”‚   â”œâ”€â”€ scraping_config/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â”‚   â””â”€â”€ submission_config/
â”‚   â”‚   â”‚       â””â”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ model/                      # Model scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt_generator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt_tuner.py
â”‚   â”‚   â”‚   â””â”€â”€ result_validator.py
â”‚   â”‚   â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â”‚   â”‚   â””â”€â”€ eda.ipynb
â”‚   â”‚   â”œâ”€â”€ prompts/                    # Prompt engineering scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_prompt.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt_improving.py
â”‚   â”‚   â”‚   â””â”€â”€ validation_prompt.py
â”‚   â”‚   â”œâ”€â”€ scraping/                   # Web scraping logic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ financial_source_finder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ scraping_challenge.py
â”‚   â”‚   â”‚   â””â”€â”€ web_scarper.py
â”‚   â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ submission/                 # Submission-related code
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ submission.py
â”‚   â”‚   â”œâ”€â”€ main.py                     # Main entry point
â”‚   â”‚   â”œâ”€â”€ submit.py                   # Submission script
â”‚   â”‚   â””â”€â”€ utils.py                    # Shared utility functions
â”œâ”€â”€ submission/                         # Submission files
â”‚   â”œâ”€â”€ submission.csv                  # Submission CSV file
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                             # Project license
â”œâ”€â”€ .gitlab-ci.yml                      # GitLab CI/CD configuration
â”œâ”€â”€ .pre-commit-config.yaml             # Pre-commit configuration
â”œâ”€â”€ CONTRIBUTING.md                     # Contribution guidelines
â”œâ”€â”€ discovery_approach.docx             # Discovery approach documentation
â”œâ”€â”€ LICENSE                             # Project license
â”œâ”€â”€ pyproject.toml                      # Project metadata and dependencies
â””â”€â”€ README.md                           # Project documentation

```



This document briefly describes the main functions of the Python script for the automatic search of financial sources for multinational enterprises (MNEs).

## Project Objective

The script aims to automatically identify, for a provided list of companies:

1. The **direct URL** to the most recent and specific financial source (e.g., annual report PDF, SEC filing page).
2. The **fiscal year** associated with the retrieved data.

It uses an iterative approach that combines Web Scraping and AI model calls (Google Gemini) for search, validation, and automatic prompt optimization.

## Main Components (Classes)

The code is structured into the following main classes:

### `WebScraperModule`

- **Purpose:** Handle all web scraping operations.
- **Key Functions:**
  - Find the company's official website.
  - Locate the "Investor Relations" (IR) page.
  - Extract links to financial reports (PDFs, etc.) from web pages.
  - Utilizes `requests` and `BeautifulSoup`, with retry handling and user-agent rotation.

### `PromptGenerator`

- **Purpose:** Dynamically create and optimize prompts sent to the AI for source searching.
- **Key Functions:**
  - Generates the initial prompt based on a template and company-specific information (if available).
  - Iteratively modifies the prompt (`optimize_prompt`) based on feedback received from the `Validator`, asking another AI instance to suggest improvements.

### `FinancialSourceFinder`

- **Purpose:** Orchestrate the entire workflow.
- **Key Functions:**
  - Initializes the other modules.
  - Loads the list of companies from a CSV file.
  - Manages parallel execution (`run`, `ThreadPoolExecutor`) for each company (`process_company`).
  - Coordinates the search â†’ validation â†’ optimization loop.
  - Saves the final results (`save_results`) in a CSV file.

## General Workflow

For each company in the input list:

1. **(Setup):** An initial prompt is generated and preliminary scraping is performed to gather reference data.
2. **(Iterative Loop - max `N` times):**
   - **Search:** The AI (`gemini-pro`) is queried with the current prompt to find the URL and year.
     - **Case 1**: The url is valid -> Save in a specific Folder
     - **Case 2**: The url is not valid -> Re-Query the model with the errors
     If after some retry the url is not identified try another approach:
     - **Generate a Scraping Script with AI**: Not yet implemented
        It will query the model in order to generate a script to scrape financial data and save in the specific folder
    If after some retry the url is not valid try another approach:
    - **Naive search** Not yet implemented
   - **Parsing:** The AIâ€™s JSON response is parsed.

   - **Validation:** If the AI returned a result
     - Is the URL accessible?
     - Relevant?
     - Specific?
     - Is the year correct and recent?
   - **Decision:**
     - If **Validated**: The result is accepted and the loop stops for that company.
     - If **Not Validated**: The `PromptGenerator` uses the validator's feedback to ask the AI to *optimize* the prompt. The loop restarts from step (a) with the new prompt.
3. **(Output):** The result (validated or the latest obtained after N iterations) is saved along with the validation status, received feedback, and scraping data.
4. **Generalization**: If more than one url is valid, generalize the code in order to give more than 1 ulr.
5. **(Final Save):** All results are consolidated and saved into a CSV file.
